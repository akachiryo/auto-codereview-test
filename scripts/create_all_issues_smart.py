#!/usr/bin/env python3
"""
GitHub Issues å…¨è‡ªå‹•ä½œæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ v4.1 (SMART)
ã™ã¹ã¦ã®Issueã‚’1ã¤ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§å‹•çš„ã«å‡¦ç†
ä¾å­˜é–¢ä¿‚ï¼šæ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã¿ä½¿ç”¨
"""

import os
import requests
import csv
import time
import sys
import math
import datetime
import random
from typing import Dict, List, Optional, Tuple
# from concurrent.futures import ThreadPoolExecutor, as_completed  # ä¸¦åˆ—å‡¦ç†ç„¡åŠ¹åŒ–
import threading

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’å–å¾—
TEAM_SETUP_TOKEN = os.environ.get('TEAM_SETUP_TOKEN')
GITHUB_REPOSITORY = os.environ.get('GITHUB_REPOSITORY')

# GitHubå…¬å¼ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œè¨­å®š
# å‚è€ƒ: https://docs.github.com/en/rest/using-the-rest-api/rate-limits-for-the-rest-api
# - Primary: 5,000 requests/hour (authenticated)
# - Secondary: 80 content-creating requests/minute
# - Recommendation: 1 second minimum between content-creating requests
PARALLEL_WORKERS = 1     # é †ç•ªä¿æŒã®ãŸã‚ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«å‡¦ç†
REQUEST_DELAY = 1.0      # 1ç§’é–“éš”ï¼ˆGitHubæ¨å¥¨æœ€å°å€¤ï¼‰
BATCH_SIZE = 10          # ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆ10ä»¶ãšã¤å‡¦ç†ï¼‰
BATCH_PAUSE = 15.0       # ãƒãƒƒãƒé–“ã®ä¼‘æ†©ï¼ˆ15ç§’ï¼‰
RETRY_DELAY = 120.0      # ãƒªãƒˆãƒ©ã‚¤é–“éš”ï¼ˆ2åˆ†ï¼‰
MAX_RETRIES = 15         # æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°
SECONDARY_LIMIT_DELAY = 300.0  # ã‚»ã‚«ãƒ³ãƒ€ãƒªåˆ¶é™æ™‚ã®å¾…æ©Ÿæ™‚é–“ï¼ˆ5åˆ†ï¼‰

if not TEAM_SETUP_TOKEN or not GITHUB_REPOSITORY:
    raise ValueError("TEAM_SETUP_TOKEN and GITHUB_REPOSITORY environment variables are required")

REPO_OWNER, REPO_NAME = GITHUB_REPOSITORY.split('/')

# GitHub APIè¨­å®š
API_BASE = 'https://api.github.com'
REST_HEADERS = {
    'Authorization': f'token {TEAM_SETUP_TOKEN}',
    'Accept': 'application/vnd.github.v3+json',
    'X-GitHub-Api-Version': '2022-11-28'
}

# GraphQL APIè¨­å®š
GRAPHQL_URL = 'https://api.github.com/graphql'
GRAPHQL_HEADERS = {
    'Authorization': f'Bearer {TEAM_SETUP_TOKEN}',
    'Content-Type': 'application/json'
}

# ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ­ãƒ¼ã‚«ãƒ«ã‚»ãƒƒã‚·ãƒ§ãƒ³
thread_local = threading.local()

def get_session():
    """ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ­ãƒ¼ã‚«ãƒ«ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’å–å¾—"""
    if not hasattr(thread_local, "session"):
        thread_local.session = requests.Session()
        thread_local.session.headers.update(REST_HEADERS)
    return thread_local.session

def check_rate_limit_headers(response):
    """ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯ã—ã€æƒ…å ±ã‚’è¡¨ç¤º"""
    headers = response.headers
    remaining = headers.get('x-ratelimit-remaining')
    limit = headers.get('x-ratelimit-limit') 
    reset_timestamp = headers.get('x-ratelimit-reset')
    
    if remaining and limit:
        remaining_pct = (int(remaining) / int(limit)) * 100
        if remaining_pct < 20:  # 20%ä»¥ä¸‹ã®å ´åˆè­¦å‘Š
            if reset_timestamp:
                import datetime
                reset_time = datetime.datetime.fromtimestamp(int(reset_timestamp))
                print(f"  âš ï¸ Rate limit warning: {remaining}/{limit} remaining ({remaining_pct:.1f}%), resets at {reset_time.strftime('%H:%M:%S')}")
            else:
                print(f"  âš ï¸ Rate limit warning: {remaining}/{remaining} remaining ({remaining_pct:.1f}%)")
        elif int(remaining) % 100 == 0:  # 100ã®å€æ•°ã§æƒ…å ±è¡¨ç¤º
            print(f"  ğŸ“Š Rate limit status: {remaining}/{limit} remaining ({remaining_pct:.1f}%)")
    
    return {
        'remaining': int(remaining) if remaining else None,
        'limit': int(limit) if limit else None,
        'reset': int(reset_timestamp) if reset_timestamp else None
    }

def load_all_csv_data() -> Tuple[List[Dict], List[Dict], List[Dict]]:
    """å…¨ã¦ã®CSVãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
    print("ğŸ“Š Loading all CSV data...")
    
    # ã‚¿ã‚¹ã‚¯Issues
    task_issues = []
    task_csv_path = 'data/tasks_for_issues.csv'
    if os.path.exists(task_csv_path):
        with open(task_csv_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            task_issues = [row for row in reader if row.get('title', '').strip()]
    
    # ãƒ†ã‚¹ãƒˆIssues
    test_issues = []
    test_csv_path = 'data/tests_for_issues.csv'
    if os.path.exists(test_csv_path):
        with open(test_csv_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            test_issues = [row for row in reader if row.get('title', '').strip()]
    
    # KPTIssues
    kpt_issues = []
    kpt_csv_path = 'data/kpt_for_issues.csv'
    if os.path.exists(kpt_csv_path):
        with open(kpt_csv_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            kpt_issues = [row for row in reader if row.get('title', '').strip()]
    
    print(f"ğŸ“‹ Loaded: {len(task_issues)} task issues, {len(test_issues)} test issues, {len(kpt_issues)} KPT issues")
    print(f"ğŸ“Š Total: {len(task_issues) + len(test_issues) + len(kpt_issues)} issues to create")
    
    return task_issues, test_issues, kpt_issues

def calculate_batches(total_count: int, batch_size: int) -> int:
    """å¿…è¦ãªãƒãƒƒãƒæ•°ã‚’è¨ˆç®—"""
    return math.ceil(total_count / batch_size)

def create_single_issue(issue_data: Dict, index: int, total: int, issue_type: str) -> Optional[Dict]:
    """å˜ä¸€ã®Issueã‚’ä½œæˆï¼ˆãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãï¼‰"""
    session = get_session()
    
    # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å›é¿ã®ãŸã‚ã®ãƒ‡ã‚£ãƒ¬ã‚¤
    if index > 0:
        time.sleep(REQUEST_DELAY)
    
    for attempt in range(MAX_RETRIES):
        try:
            response = session.post(
                f"{API_BASE}/repos/{GITHUB_REPOSITORY}/issues",
                json=issue_data,
                timeout=30
            )
            
            if response.status_code == 201:
                issue = response.json()
                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯
                check_rate_limit_headers(response)
                
                if attempt > 0:
                    print(f"  âœ… {issue_type} ({index + 1}/{total}) [retry {attempt}]: {issue_data['title'][:50]}...")
                else:
                    print(f"  âœ… {issue_type} ({index + 1}/{total}): {issue_data['title'][:50]}...")
                return issue
            
            elif response.status_code == 403:
                # GitHubæ¨å¥¨: ã‚»ã‚«ãƒ³ãƒ€ãƒªãƒ¬ãƒ¼ãƒˆåˆ¶é™ã®å¯èƒ½æ€§
                retry_after = response.headers.get('retry-after')
                if retry_after:
                    wait_time = int(retry_after)
                    print(f"  â³ Rate limit (retry-after: {wait_time}s) ({index + 1}/{total}) [attempt {attempt + 1}]")
                else:
                    # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ• with jitter
                    base_delay = RETRY_DELAY if attempt == 0 else SECONDARY_LIMIT_DELAY
                    jitter = random.uniform(0.8, 1.2)
                    wait_time = int(base_delay * (2 ** (attempt // 2)) * jitter)
                    print(f"  â³ Rate limit hit ({index + 1}/{total}) [attempt {attempt + 1}], waiting {wait_time}s...")
                time.sleep(wait_time)
                continue
                
            elif response.status_code >= 500:
                print(f"  ğŸ”„ Server error ({response.status_code}) ({index + 1}/{total}) [attempt {attempt + 1}]...")
                time.sleep(RETRY_DELAY * (attempt + 1))
                continue
            
            else:
                print(f"  âŒ {issue_type} failed ({index + 1}/{total}): {response.status_code} - {response.text[:100]}")
                break
                
        except Exception as e:
            print(f"  âŒ {issue_type} exception ({index + 1}/{total}) [attempt {attempt + 1}]: {str(e)}")
            if attempt < MAX_RETRIES - 1:
                time.sleep(RETRY_DELAY * (attempt + 1))
                continue
    
    return None

def create_issues_batch(issues_data: List[Tuple], batch_num: int, total_batches: int) -> Tuple[List[Dict], List[Tuple]]:
    """1ã¤ã®ãƒãƒƒãƒã§Issueã‚’ä½œæˆï¼ˆå¤±æ•—ã—ãŸã‚‚ã®ã‚’è¿”ã™ï¼‰"""
    created_issues = []
    failed_issues = []
    
    if not issues_data:
        return created_issues, failed_issues
    
    print(f"ğŸš€ Processing batch {batch_num}/{total_batches} ({len(issues_data)} issues)")
    
    # ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«å®Ÿè¡Œï¼ˆé †ç•ªä¿æŒã®ãŸã‚ï¼‰
    for i, (issue_data, issue_type) in enumerate(issues_data):
        try:
            issue = create_single_issue(issue_data, i, len(issues_data), issue_type)
            if issue:
                created_issues.append(issue)
            else:
                failed_issues.append((issue_data, issue_type))
        except Exception as e:
            print(f"  âŒ Exception: {str(e)}")
            failed_issues.append((issue_data, issue_type))
    
    print(f"ğŸ“Š Batch {batch_num} result: {len(created_issues)}/{len(issues_data)} issues created, {len(failed_issues)} failed")
    return created_issues, failed_issues

# Custom field functions removed - using labels instead

def add_issue_to_project_fast(project_id: str, issue: Dict) -> Optional[str]:
    """é«˜é€Ÿã§Issueã‚’Projectã«è¿½åŠ ã—ã€ã‚¢ã‚¤ãƒ†ãƒ IDã‚’è¿”ã™"""
    query = """
    mutation($projectId: ID!, $contentId: ID!) {
        addProjectV2ItemById(input: {projectId: $projectId, contentId: $contentId}) {
            item { id }
        }
    }
    """
    
    variables = {
        'projectId': project_id,
        'contentId': issue['node_id']
    }
    
    payload = {'query': query, 'variables': variables}
    
    try:
        response = requests.post(
            GRAPHQL_URL, 
            json=payload, 
            headers=GRAPHQL_HEADERS,
            timeout=30
        )
        
        if response.status_code == 200:
            data = response.json()
            if 'errors' not in data and 'data' in data:
                return data['data']['addProjectV2ItemById']['item']['id']
        return None
    except:
        return None

def link_issues_to_projects(task_issues: List[Dict], test_issues: List[Dict], kpt_issues: List[Dict], project_ids: Dict[str, str]):
    """Issueã‚’Projectsã«ãƒªãƒ³ã‚¯"""
    print("\nğŸ”— Linking issues to projects...")
    
    def link_batch(issues: List[Dict], project_id: str, project_name: str, issue_type: str):
        if not issues or not project_id:
            return 0
        
        print(f"  ğŸ“Œ Linking {len(issues)} {issue_type} issues to {project_name}")
        success_count = 0
        
        for i, issue in enumerate(issues):
            try:
                item_id = add_issue_to_project_fast(project_id, issue)
                if item_id:
                    success_count += 1
                
                if (i + 1) % 20 == 0:
                    print(f"    âœ… Linked {i + 1}/{len(issues)} to {project_name}")
            except Exception as e:
                print(f"    âŒ Link exception: {str(e)}")
            time.sleep(0.1)  # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒªãƒ³ã‚¯ã‚‚å°‘ã—é–“éš”ã‚’ç©ºã‘ã‚‹
        
        print(f"  ğŸ“Š {project_name}: {success_count}/{len(issues)} issues linked")
        return success_count
    
    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«ãƒªãƒ³ã‚¯
    task_linked = link_batch(
        task_issues, 
        project_ids.get('ã‚¤ãƒã‚³ã‚³SNSï¼ˆã‚¿ã‚¹ã‚¯ï¼‰'), 
        'ã‚¤ãƒã‚³ã‚³SNSï¼ˆã‚¿ã‚¹ã‚¯ï¼‰',
        'task'
    )
    
    test_linked = link_batch(
        test_issues,
        project_ids.get('ã‚¤ãƒã‚³ã‚³SNSï¼ˆãƒ†ã‚¹ãƒˆï¼‰'),
        'ã‚¤ãƒã‚³ã‚³SNSï¼ˆãƒ†ã‚¹ãƒˆï¼‰',
        'test'
    )
    
    kpt_linked = link_batch(
        kpt_issues,
        project_ids.get('ã‚¤ãƒã‚³ã‚³SNSï¼ˆKPTï¼‰'),
        'ã‚¤ãƒã‚³ã‚³SNSï¼ˆKPTï¼‰',
        'kpt'
    )
    
    return task_linked, test_linked, kpt_linked

def load_project_ids() -> Dict[str, str]:
    """ä¿å­˜ã•ã‚ŒãŸãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆIDã‚’èª­ã¿è¾¼ã¿"""
    project_ids = {}
    try:
        with open('project_ids.txt', 'r', encoding='utf-8') as f:
            for line in f:
                if ':' in line:
                    title, project_id = line.strip().split(':', 1)
                    project_ids[title] = project_id
        print(f"ğŸ“‚ Loaded {len(project_ids)} project IDs")
    except FileNotFoundError:
        print("âš ï¸ project_ids.txt not found. Issues will be created but not linked to projects.")
    
    return project_ids

def prepare_issue_data(issues: List[Dict], labels: List[str], issue_type: str) -> List[Tuple[Dict, str]]:
    """Issueä½œæˆç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ï¼ˆç•ªå·ä»˜ãã‚¿ã‚¤ãƒˆãƒ«ï¼‰"""
    issue_requests = []
    
    for index, row in enumerate(issues, 1):
        title = row.get('title', '').strip()
        body = row.get('body', '').strip()
        
        if not title:
            continue
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã«ç•ªå·ã‚’è¿½åŠ ï¼ˆæ—¢ã«ç•ªå·ãŒã‚ã‚‹å ´åˆã¯ç½®ãæ›ãˆï¼‰
        if issue_type == 'task':
            # ã€Œã‚¿ã‚¹ã‚¯ã€ã§å§‹ã¾ã‚‹å ´åˆã¯ã€ç•ªå·ã‚’ç½®ãæ›ãˆ
            if title.startswith('ã‚¿ã‚¹ã‚¯'):
                # ã€Œã‚¿ã‚¹ã‚¯ã€ã®å¾Œã®æ•°å­—ã‚„ã‚³ãƒ­ãƒ³ã‚’å‰Šé™¤ã—ã€æœ¬æ–‡ã‚’æŠ½å‡º
                import re
                match = re.match(r'ã‚¿ã‚¹ã‚¯[\d\s:.]*(.+)', title)
                if match:
                    clean_title = match.group(1).strip()
                else:
                    clean_title = title
                numbered_title = f"ã‚¿ã‚¹ã‚¯{index:03d}: {clean_title}"
            else:
                numbered_title = f"ã‚¿ã‚¹ã‚¯{index:03d}: {title}"
        elif issue_type == 'test':
            # ã€Œãƒ†ã‚¹ãƒˆã€ã§å§‹ã¾ã‚‹å ´åˆã¯ã€ç•ªå·ã‚’ç½®ãæ›ãˆ
            if title.startswith('ãƒ†ã‚¹ãƒˆ'):
                import re
                match = re.match(r'ãƒ†ã‚¹ãƒˆ[\d\s:.]*(.+)', title)
                if match:
                    clean_title = match.group(1).strip()
                else:
                    clean_title = title
                numbered_title = f"ãƒ†ã‚¹ãƒˆ{index:03d}: {clean_title}"
            else:
                numbered_title = f"ãƒ†ã‚¹ãƒˆ{index:03d}: {title}"
        else:  # KPT issues
            # KPT issuesã¯æ—¢ã«é©åˆ‡ãªç•ªå·ä»˜ã‘ãŒã•ã‚Œã¦ã„ã‚‹ã®ã§ãã®ã¾ã¾ä½¿ç”¨
            numbered_title = title
            
        # CSVã‹ã‚‰ãƒ©ãƒ™ãƒ«ã‚’å–å¾—ï¼ˆ"task,Required"ã®ã‚ˆã†ãªå½¢å¼ã«å¯¾å¿œï¼‰
        labels_str = row.get('labels', '').strip()
        if labels_str.startswith('"') and labels_str.endswith('"'):
            labels_str = labels_str[1:-1]  # ã‚¯ã‚©ãƒ¼ãƒˆã‚’é™¤å»
        existing_labels = [label.strip() for label in labels_str.split(',') if label.strip()]
        
        # è¿½åŠ ãƒ©ãƒ™ãƒ«ãŒã‚ã‚‹å ´åˆã¯ãƒãƒ¼ã‚¸
        all_labels = list(set(existing_labels + labels))
        
        issue_data = {
            'title': numbered_title,
            'body': body,
            'labels': all_labels
        }
        
        issue_requests.append((issue_data, issue_type))
    
    return issue_requests

def retry_failed_issues(failed_issues: List[Tuple], max_retry_rounds: int = 2) -> List[Dict]:
    """å¤±æ•—ã—ãŸissueã‚’ãƒªãƒˆãƒ©ã‚¤ã™ã‚‹"""
    if not failed_issues:
        return []
    
    print(f"\nğŸ”„ Retrying {len(failed_issues)} failed issues...")
    
    retry_created = []
    remaining_failed = failed_issues.copy()
    
    for round_num in range(max_retry_rounds):
        if not remaining_failed:
            break
            
        print(f"  ğŸ” Retry round {round_num + 1}/{max_retry_rounds}: {len(remaining_failed)} issues")
        
        # ãƒªãƒˆãƒ©ã‚¤å‰ã«é•·ã‚ã®ä¼‘æ†©
        time.sleep(3.0)
        
        current_round_created, current_round_failed = create_issues_batch(
            remaining_failed, round_num + 1, max_retry_rounds
        )
        
        retry_created.extend(current_round_created)
        remaining_failed = current_round_failed
        
        # æ¬¡ã®ãƒ©ã‚¦ãƒ³ãƒ‰ã¾ã§ã®ä¼‘æ†©
        if remaining_failed and round_num < max_retry_rounds - 1:
            print(f"    â³ Waiting before next retry round...")
            time.sleep(5.0)
    
    if remaining_failed:
        print(f"  âš ï¸ {len(remaining_failed)} issues could not be created after all retries")
        print("  Failed issues:")
        for issue_data, issue_type in remaining_failed[:5]:  # æœ€åˆã®5å€‹ã ã‘è¡¨ç¤º
            print(f"    - {issue_type}: {issue_data['title'][:50]}...")
        if len(remaining_failed) > 5:
            print(f"    ... and {len(remaining_failed) - 5} more")
    
    print(f"  âœ… Retry success: {len(retry_created)} issues created")
    return retry_created

def check_initial_rate_limit():
    """åˆæœŸãƒ¬ãƒ¼ãƒˆåˆ¶é™çŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯"""
    try:
        response = requests.get(f"{API_BASE}/rate_limit", headers=REST_HEADERS, timeout=10)
        if response.status_code == 200:
            data = response.json()
            core = data.get('resources', {}).get('core', {})
            remaining = core.get('remaining', 0)
            limit = core.get('limit', 0)
            reset_timestamp = core.get('reset', 0)
            
            if reset_timestamp:
                import datetime
                reset_time = datetime.datetime.fromtimestamp(reset_timestamp)
                print(f"ğŸ“Š Initial rate limit: {remaining}/{limit} requests remaining")
                print(f"ğŸ”„ Rate limit resets at: {reset_time.strftime('%Y-%m-%d %H:%M:%S')}")
                
                if remaining < 100:
                    print(f"âš ï¸ Warning: Low rate limit remaining ({remaining}). Consider waiting until reset.")
            return remaining
    except Exception as e:
        print(f"âš ï¸ Could not check rate limit: {str(e)}")
    return None

def estimate_completion_time(total_issues):
    """å®Œäº†äºˆæƒ³æ™‚åˆ»ã‚’è¨ˆç®—"""
    # 1ç§’é–“éš” + ãƒãƒƒãƒä¼‘æ†©ã‚’è€ƒæ…®
    issues_per_batch = BATCH_SIZE
    batches = math.ceil(total_issues / issues_per_batch)
    
    time_per_issue = REQUEST_DELAY
    time_for_issues = total_issues * time_per_issue
    time_for_batch_pauses = (batches - 1) * BATCH_PAUSE
    
    total_seconds = time_for_issues + time_for_batch_pauses
    minutes = int(total_seconds // 60)
    seconds = int(total_seconds % 60)
    
    print(f"â±ï¸ Estimated completion time: {minutes}m {seconds}s ({batches} batches)")
    return total_seconds

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=" * 70)
    print("ğŸ§  SMART ALL-IN-ONE ISSUE CREATOR v4.4 (GitHub Rate Limit Optimized)")
    print("=" * 70)
    print(f"ğŸ“¦ Repository: {GITHUB_REPOSITORY}")
    print(f"â° Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ”§ Script: create_all_issues_smart.py v4.4")
    print(f"âš™ï¸ GitHub Rate Limit Configuration:")
    print(f"  â€¢ Request Delay: {REQUEST_DELAY}s (GitHub minimum: 1s)")
    print(f"  â€¢ Batch Size: {BATCH_SIZE} (under 80/min limit)")
    print(f"  â€¢ Batch Pause: {BATCH_PAUSE}s")
    print(f"  â€¢ Retry Delay: {RETRY_DELAY}s (secondary limit handling)")
    print(f"  â€¢ Max Retries: {MAX_RETRIES}")
    print("=" * 70)
    
    # åˆæœŸãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒã‚§ãƒƒã‚¯
    check_initial_rate_limit()
    
    start_time = time.time()
    
    try:
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        task_data, test_data, kpt_data = load_all_csv_data()
        total_issues = len(task_data) + len(test_data) + len(kpt_data)
        
        if total_issues == 0:
            print("âš ï¸ No issues found in CSV files")
            return 1
        
        # ãƒãƒƒãƒè¨ˆç®—ã¨æ™‚é–“äºˆæƒ³
        total_batches = calculate_batches(total_issues, BATCH_SIZE)
        print(f"\nğŸ“Š Processing plan:")
        print(f"  â€¢ Total issues: {total_issues}")
        print(f"  â€¢ Batch size: {BATCH_SIZE}")
        print(f"  â€¢ Total batches: {total_batches}")
        
        # å®Œäº†äºˆæƒ³æ™‚åˆ»ã‚’è¡¨ç¤º
        estimate_completion_time(total_issues)
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆIDã‚’èª­ã¿è¾¼ã¿
        project_ids = load_project_ids()
        
        # Issueä½œæˆç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™
        task_requests = prepare_issue_data(task_data, [], 'task')  # CSVã«ãƒ©ãƒ™ãƒ«ãŒå«ã¾ã‚Œã¦ã„ã‚‹
        test_requests = prepare_issue_data(test_data, [], 'test')  # CSVã«ãƒ©ãƒ™ãƒ«ãŒå«ã¾ã‚Œã¦ã„ã‚‹
        kpt_requests = prepare_issue_data(kpt_data, [], 'kpt')  # CSVã«ãƒ©ãƒ™ãƒ«ãŒå«ã¾ã‚Œã¦ã„ã‚‹
        all_requests = task_requests + test_requests + kpt_requests
        
        print(f"\nğŸ“‹ Prepared requests: {len(all_requests)} issues")
        
        # ãƒãƒƒãƒå‡¦ç†
        all_created_issues = []
        task_created = []
        test_created = []
        kpt_created = []
        
        for batch_num in range(total_batches):
            start_idx = batch_num * BATCH_SIZE
            end_idx = min(start_idx + BATCH_SIZE, len(all_requests))
            batch_requests = all_requests[start_idx:end_idx]
            
            print(f"\nğŸ”„ Batch {batch_num + 1}/{total_batches}: Processing issues {start_idx + 1}-{end_idx}")
            
            batch_created, batch_failed = create_issues_batch(batch_requests, batch_num + 1, total_batches)
            all_created_issues.extend(batch_created)
            
            # å¤±æ•—ã—ãŸã‚‚ã®ã‚’é›†ç´„
            if batch_failed:
                print(f"  ğŸ“ {len(batch_failed)} issues failed in this batch, will retry later...")
                if 'all_failed_issues' not in locals():
                    all_failed_issues = []
                all_failed_issues.extend(batch_failed)
            
            # ã‚¿ã‚¹ã‚¯/ãƒ†ã‚¹ãƒˆ/KPTåˆ¥ã«åˆ†é¡
            for issue in batch_created:
                issue_labels = [label['name'] for label in issue.get('labels', [])]
                if 'task' in issue_labels:
                    task_created.append(issue)
                elif 'kpt' in issue_labels:
                    kpt_created.append(issue)
                else:
                    test_created.append(issue)
            
            # ãƒãƒƒãƒé–“ã®ä¼‘æ†©ï¼ˆGitHubæ¨å¥¨ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
            if batch_num < total_batches - 1:
                print(f"  â³ Batch pause ({BATCH_PAUSE}s)...")
                time.sleep(BATCH_PAUSE)
        
        # å¤±æ•—ã—ãŸã‚‚ã®ã®ãƒªãƒˆãƒ©ã‚¤
        retry_created = []
        if 'all_failed_issues' in locals() and all_failed_issues:
            retry_created = retry_failed_issues(all_failed_issues)
            all_created_issues.extend(retry_created)
            
            # ãƒªãƒˆãƒ©ã‚¤ã§ä½œæˆã•ã‚ŒãŸã‚‚ã®ã‚‚åˆ†é¡
            for issue in retry_created:
                issue_labels = [label['name'] for label in issue.get('labels', [])]
                if 'task' in issue_labels:
                    task_created.append(issue)
                elif 'kpt' in issue_labels:
                    kpt_created.append(issue)
                else:
                    test_created.append(issue)
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒªãƒ³ã‚¯
        task_linked, test_linked, kpt_linked = link_issues_to_projects(task_created, test_created, kpt_created, project_ids)
        
        # çµæœã‚µãƒãƒªãƒ¼
        end_time = time.time()
        execution_time = end_time - start_time
        
        print(f"\n" + "=" * 60)
        print("ğŸ‰ SMART PROCESSING COMPLETED!")
        print("=" * 60)
        print(f"ğŸ“Š Results:")
        print(f"  â€¢ Task issues created: {len(task_created)}")
        print(f"  â€¢ Test issues created: {len(test_created)}")
        print(f"  â€¢ KPT issues created: {len(kpt_created)}")
        print(f"  â€¢ Total issues created: {len(all_created_issues)}")
        if retry_created:
            print(f"  â€¢ Retry issues created: {len(retry_created)}")
        print(f"  â€¢ Task issues linked: {task_linked}")
        print(f"  â€¢ Test issues linked: {test_linked}")
        print(f"  â€¢ KPT issues linked: {kpt_linked}")
        final_failed = len(all_failed_issues) - len(retry_created) if 'all_failed_issues' in locals() else 0
        if final_failed > 0:
            print(f"  â€¢ Final failed issues: {final_failed}")
        print(f"  â€¢ Success rate: {(len(all_created_issues)/total_issues*100):.1f}%")
        print(f"â±ï¸ Performance:")
        print(f"  â€¢ Execution time: {execution_time:.1f} seconds")
        print(f"  â€¢ Average per issue: {(execution_time/len(all_created_issues)):.2f}s")
        
        # çµæœä¿å­˜
        with open('smart_issue_creation_result.txt', 'w', encoding='utf-8') as f:
            f.write(f"Smart Issue Creation Results\n")
            f.write(f"Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Task issues: {len(task_created)}\n")
            f.write(f"Test issues: {len(test_created)}\n")
            f.write(f"KPT issues: {len(kpt_created)}\n")
            f.write(f"Total: {len(all_created_issues)}\n")
            if retry_created:
                f.write(f"Retry issues: {len(retry_created)}\n")
            final_failed = len(all_failed_issues) - len(retry_created) if 'all_failed_issues' in locals() else 0
            if final_failed > 0:
                f.write(f"Final failed issues: {final_failed}\n")
            f.write(f"Execution time: {execution_time:.1f}s\n")
            f.write(f"Success rate: {(len(all_created_issues)/total_issues*100):.1f}%\n")
        
        return 0
        
    except Exception as e:
        print(f"\nğŸ’¥ Unexpected error: {str(e)}")
        print(f"ğŸ”§ Error type: {type(e).__name__}")
        return 1

if __name__ == '__main__':
    exit(main())
