#!/usr/bin/env python3
"""
GitHub Issues å…¨è‡ªå‹•ä½œæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ v4.1 (SMART)
ã™ã¹ã¦ã®Issueã‚’1ã¤ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§å‹•çš„ã«å‡¦ç†
ä¾å­˜é–¢ä¿‚ï¼šæ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã¿ä½¿ç”¨
"""

import os
import requests
import csv
import time
import sys
from typing import Dict, List, Optional, Tuple
# from concurrent.futures import ThreadPoolExecutor, as_completed  # ä¸¦åˆ—å‡¦ç†ç„¡åŠ¹åŒ–
import threading
import math

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’å–å¾—
TEAM_SETUP_TOKEN = os.environ.get('TEAM_SETUP_TOKEN')
GITHUB_REPOSITORY = os.environ.get('GITHUB_REPOSITORY')

# å‹•çš„è¨­å®š
PARALLEL_WORKERS = 1     # é †ç•ªä¿æŒã®ãŸã‚ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«å‡¦ç†
REQUEST_DELAY = 0.5      # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å›é¿ã®ãŸã‚ã®ãƒ‡ã‚£ãƒ¬ã‚¤
BATCH_SIZE = 30          # ãƒãƒƒãƒã‚µã‚¤ã‚º
BURST_LIMIT = 10         # ãƒãƒ¼ã‚¹ãƒˆãƒªãƒŸãƒƒãƒˆ
RETRY_DELAY = 2.0        # ãƒªãƒˆãƒ©ã‚¤é–“éš”
MAX_RETRIES = 3          # æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°

if not TEAM_SETUP_TOKEN or not GITHUB_REPOSITORY:
    raise ValueError("TEAM_SETUP_TOKEN and GITHUB_REPOSITORY environment variables are required")

REPO_OWNER, REPO_NAME = GITHUB_REPOSITORY.split('/')

# GitHub APIè¨­å®š
API_BASE = 'https://api.github.com'
REST_HEADERS = {
    'Authorization': f'token {TEAM_SETUP_TOKEN}',
    'Accept': 'application/vnd.github.v3+json',
    'X-GitHub-Api-Version': '2022-11-28'
}

# GraphQL APIè¨­å®š
GRAPHQL_URL = 'https://api.github.com/graphql'
GRAPHQL_HEADERS = {
    'Authorization': f'Bearer {TEAM_SETUP_TOKEN}',
    'Content-Type': 'application/json'
}

# ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ­ãƒ¼ã‚«ãƒ«ã‚»ãƒƒã‚·ãƒ§ãƒ³
thread_local = threading.local()

def get_session():
    """ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ­ãƒ¼ã‚«ãƒ«ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’å–å¾—"""
    if not hasattr(thread_local, "session"):
        thread_local.session = requests.Session()
        thread_local.session.headers.update(REST_HEADERS)
    return thread_local.session

def load_all_csv_data() -> Tuple[List[Dict], List[Dict], List[Dict]]:
    """å…¨ã¦ã®CSVãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
    print("ğŸ“Š Loading all CSV data...")
    
    # ã‚¿ã‚¹ã‚¯Issues
    task_issues = []
    task_csv_path = 'data/tasks_for_issues.csv'
    if os.path.exists(task_csv_path):
        with open(task_csv_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            task_issues = [row for row in reader if row.get('title', '').strip()]
    
    # ãƒ†ã‚¹ãƒˆIssues
    test_issues = []
    test_csv_path = 'data/tests_for_issues.csv'
    if os.path.exists(test_csv_path):
        with open(test_csv_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            test_issues = [row for row in reader if row.get('title', '').strip()]
    
    # KPTIssues
    kpt_issues = []
    kpt_csv_path = 'data/kpt_for_issues.csv'
    if os.path.exists(kpt_csv_path):
        with open(kpt_csv_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            kpt_issues = [row for row in reader if row.get('title', '').strip()]
    
    print(f"ğŸ“‹ Loaded: {len(task_issues)} task issues, {len(test_issues)} test issues, {len(kpt_issues)} KPT issues")
    print(f"ğŸ“Š Total: {len(task_issues) + len(test_issues) + len(kpt_issues)} issues to create")
    
    # ãƒ‡ãƒãƒƒã‚°: æœ€åˆã®æ•°ä»¶ã®difficultyã‚’ç¢ºèª
    if task_issues:
        print(f"ğŸ” Debug: First few task difficulties:")
        for i, task in enumerate(task_issues[:5]):
            print(f"    Task {i+1}: {task.get('title', 'No title')[:50]}... -> difficulty: '{task.get('difficulty', 'None')}'")
        
        # å„é›£æ˜“åº¦ã®æ•°ã‚‚ã‚«ã‚¦ãƒ³ãƒˆ
        difficulties = {}
        for task in task_issues:
            diff = task.get('difficulty', '')
            difficulties[diff] = difficulties.get(diff, 0) + 1
        print(f"ğŸ” Debug: Difficulty distribution: {difficulties}")
    
    return task_issues, test_issues, kpt_issues

def calculate_batches(total_count: int, batch_size: int) -> int:
    """å¿…è¦ãªãƒãƒƒãƒæ•°ã‚’è¨ˆç®—"""
    return math.ceil(total_count / batch_size)

def create_single_issue(issue_data: Dict, index: int, total: int, issue_type: str) -> Optional[Dict]:
    """å˜ä¸€ã®Issueã‚’ä½œæˆï¼ˆãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãï¼‰"""
    session = get_session()
    
    # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å›é¿ã®ãŸã‚ã®ãƒ‡ã‚£ãƒ¬ã‚¤
    if index > 0:
        time.sleep(REQUEST_DELAY)
    
    for attempt in range(MAX_RETRIES):
        try:
            response = session.post(
                f"{API_BASE}/repos/{GITHUB_REPOSITORY}/issues",
                json=issue_data,
                timeout=30
            )
            
            if response.status_code == 201:
                issue = response.json()
                if attempt > 0:
                    print(f"  âœ… {issue_type} ({index + 1}/{total}) [retry {attempt}]: {issue_data['title'][:50]}...")
                else:
                    print(f"  âœ… {issue_type} ({index + 1}/{total}): {issue_data['title'][:50]}...")
                return issue
            
            elif response.status_code == 403:
                # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
                backoff_time = RETRY_DELAY * (2 ** attempt)
                print(f"  â³ Rate limit hit ({index + 1}/{total}) [attempt {attempt + 1}], waiting {backoff_time}s...")
                time.sleep(backoff_time)
                continue
                
            elif response.status_code >= 500:
                print(f"  ğŸ”„ Server error ({response.status_code}) ({index + 1}/{total}) [attempt {attempt + 1}]...")
                time.sleep(RETRY_DELAY * (attempt + 1))
                continue
            
            else:
                print(f"  âŒ {issue_type} failed ({index + 1}/{total}): {response.status_code} - {response.text[:100]}")
                break
                
        except Exception as e:
            print(f"  âŒ {issue_type} exception ({index + 1}/{total}) [attempt {attempt + 1}]: {str(e)}")
            if attempt < MAX_RETRIES - 1:
                time.sleep(RETRY_DELAY * (attempt + 1))
                continue
    
    return None

def create_issues_batch(issues_data: List[Tuple], batch_num: int, total_batches: int) -> Tuple[List[Dict], List[Tuple]]:
    """1ã¤ã®ãƒãƒƒãƒã§Issueã‚’ä½œæˆï¼ˆå¤±æ•—ã—ãŸã‚‚ã®ã‚’è¿”ã™ï¼‰"""
    created_issues = []
    failed_issues = []
    
    if not issues_data:
        return created_issues, failed_issues
    
    print(f"ğŸš€ Processing batch {batch_num}/{total_batches} ({len(issues_data)} issues)")
    
    # ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«å®Ÿè¡Œï¼ˆé †ç•ªä¿æŒã®ãŸã‚ï¼‰
    for i, (issue_data, issue_type) in enumerate(issues_data):
        try:
            issue = create_single_issue(issue_data, i, len(issues_data), issue_type)
            if issue:
                created_issues.append(issue)
            else:
                failed_issues.append((issue_data, issue_type))
        except Exception as e:
            print(f"  âŒ Exception: {str(e)}")
            failed_issues.append((issue_data, issue_type))
    
    print(f"ğŸ“Š Batch {batch_num} result: {len(created_issues)}/{len(issues_data)} issues created, {len(failed_issues)} failed")
    return created_issues, failed_issues

# Custom field functions removed - using labels instead

def add_issue_to_project_fast(project_id: str, issue: Dict) -> Optional[str]:
    """é«˜é€Ÿã§Issueã‚’Projectã«è¿½åŠ ã—ã€ã‚¢ã‚¤ãƒ†ãƒ IDã‚’è¿”ã™"""
    query = """
    mutation($projectId: ID!, $contentId: ID!) {
        addProjectV2ItemById(input: {projectId: $projectId, contentId: $contentId}) {
            item { id }
        }
    }
    """
    
    variables = {
        'projectId': project_id,
        'contentId': issue['node_id']
    }
    
    payload = {'query': query, 'variables': variables}
    
    try:
        response = requests.post(
            GRAPHQL_URL, 
            json=payload, 
            headers=GRAPHQL_HEADERS,
            timeout=30
        )
        
        if response.status_code == 200:
            data = response.json()
            if 'errors' not in data and 'data' in data:
                return data['data']['addProjectV2ItemById']['item']['id']
        return None
    except:
        return None

def link_issues_to_projects(task_issues: List[Dict], test_issues: List[Dict], kpt_issues: List[Dict], project_ids: Dict[str, str]):
    """Issueã‚’Projectsã«ãƒªãƒ³ã‚¯"""
    print("\nğŸ”— Linking issues to projects...")
    
    def link_batch(issues: List[Dict], project_id: str, project_name: str, issue_type: str):
        if not issues or not project_id:
            return 0
        
        print(f"  ğŸ“Œ Linking {len(issues)} {issue_type} issues to {project_name}")
        success_count = 0
        
        for i, issue in enumerate(issues):
            try:
                item_id = add_issue_to_project_fast(project_id, issue)
                if item_id:
                    success_count += 1
                
                if (i + 1) % 20 == 0:
                    print(f"    âœ… Linked {i + 1}/{len(issues)} to {project_name}")
            except Exception as e:
                print(f"    âŒ Link exception: {str(e)}")
            time.sleep(0.1)  # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒªãƒ³ã‚¯ã‚‚å°‘ã—é–“éš”ã‚’ç©ºã‘ã‚‹
        
        print(f"  ğŸ“Š {project_name}: {success_count}/{len(issues)} issues linked")
        return success_count
    
    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«ãƒªãƒ³ã‚¯
    task_linked = link_batch(
        task_issues, 
        project_ids.get('ã‚¤ãƒã‚³ã‚³SNSï¼ˆã‚¿ã‚¹ã‚¯ï¼‰'), 
        'ã‚¤ãƒã‚³ã‚³SNSï¼ˆã‚¿ã‚¹ã‚¯ï¼‰',
        'task'
    )
    
    test_linked = link_batch(
        test_issues,
        project_ids.get('ã‚¤ãƒã‚³ã‚³SNSï¼ˆãƒ†ã‚¹ãƒˆï¼‰'),
        'ã‚¤ãƒã‚³ã‚³SNSï¼ˆãƒ†ã‚¹ãƒˆï¼‰',
        'test'
    )
    
    kpt_linked = link_batch(
        kpt_issues,
        project_ids.get('ã‚¤ãƒã‚³ã‚³SNSï¼ˆKPTï¼‰'),
        'ã‚¤ãƒã‚³ã‚³SNSï¼ˆKPTï¼‰',
        'kpt'
    )
    
    return task_linked, test_linked, kpt_linked

def load_project_ids() -> Dict[str, str]:
    """ä¿å­˜ã•ã‚ŒãŸãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆIDã‚’èª­ã¿è¾¼ã¿"""
    project_ids = {}
    try:
        with open('project_ids.txt', 'r', encoding='utf-8') as f:
            for line in f:
                if ':' in line:
                    title, project_id = line.strip().split(':', 1)
                    project_ids[title] = project_id
        print(f"ğŸ“‚ Loaded {len(project_ids)} project IDs")
    except FileNotFoundError:
        print("âš ï¸ project_ids.txt not found. Issues will be created but not linked to projects.")
    
    return project_ids

def prepare_issue_data(issues: List[Dict], labels: List[str]) -> List[Tuple[Dict, str]]:
    """Issueä½œæˆç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ï¼ˆç•ªå·ä»˜ãã‚¿ã‚¤ãƒˆãƒ«ï¼‰"""
    issue_requests = []
    if 'task' in labels:
        issue_type = 'task'
    elif 'test' in labels:
        issue_type = 'test'
    else:
        issue_type = 'kpt'
    
    for index, row in enumerate(issues, 1):
        title = row.get('title', '').strip()
        body = row.get('body', '').strip()
        
        if not title:
            continue
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã«ç•ªå·ã‚’è¿½åŠ ï¼ˆæ—¢ã«ç•ªå·ãŒã‚ã‚‹å ´åˆã¯ç½®ãæ›ãˆï¼‰
        if issue_type == 'task':
            # ã€Œã‚¿ã‚¹ã‚¯ã€ã§å§‹ã¾ã‚‹å ´åˆã¯ã€ç•ªå·ã‚’ç½®ãæ›ãˆ
            if title.startswith('ã‚¿ã‚¹ã‚¯'):
                # ã€Œã‚¿ã‚¹ã‚¯ã€ã®å¾Œã®æ•°å­—ã‚„ã‚³ãƒ­ãƒ³ã‚’å‰Šé™¤ã—ã€æœ¬æ–‡ã‚’æŠ½å‡º
                import re
                match = re.match(r'ã‚¿ã‚¹ã‚¯[\d\s:.]*(.+)', title)
                if match:
                    clean_title = match.group(1).strip()
                else:
                    clean_title = title
                numbered_title = f"ã‚¿ã‚¹ã‚¯{index:03d}: {clean_title}"
            else:
                numbered_title = f"ã‚¿ã‚¹ã‚¯{index:03d}: {title}"
        elif issue_type == 'test':
            # ã€Œãƒ†ã‚¹ãƒˆã€ã§å§‹ã¾ã‚‹å ´åˆã¯ã€ç•ªå·ã‚’ç½®ãæ›ãˆ
            if title.startswith('ãƒ†ã‚¹ãƒˆ'):
                import re
                match = re.match(r'ãƒ†ã‚¹ãƒˆ[\d\s:.]*(.+)', title)
                if match:
                    clean_title = match.group(1).strip()
                else:
                    clean_title = title
                numbered_title = f"ãƒ†ã‚¹ãƒˆ{index:03d}: {clean_title}"
            else:
                numbered_title = f"ãƒ†ã‚¹ãƒˆ{index:03d}: {title}"
        else:  # KPT issues
            # KPT issuesã¯æ—¢ã«é©åˆ‡ãªç•ªå·ä»˜ã‘ãŒã•ã‚Œã¦ã„ã‚‹ã®ã§ãã®ã¾ã¾ä½¿ç”¨
            numbered_title = title
            
        existing_labels = [label.strip() for label in row.get('labels', '').split(',') if label.strip()]
        
        # ã‚¿ã‚¹ã‚¯ã®å ´åˆã¯é›£æ˜“åº¦ã‚’ãƒ©ãƒ™ãƒ«ã¨ã—ã¦ä½¿ç”¨ã€ãã‚Œä»¥å¤–ã¯å¾“æ¥é€šã‚Š
        if issue_type == 'task':
            difficulty = row.get('difficulty', '').strip()
            if difficulty:
                # é›£æ˜“åº¦ã‚’ãƒ¡ã‚¤ãƒ³ãƒ©ãƒ™ãƒ«ã¨ã—ã¦ä½¿ç”¨
                all_labels = list(set(existing_labels + [difficulty]))
            else:
                # é›£æ˜“åº¦ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯CSVã®ãƒ©ãƒ™ãƒ«ã®ã¿
                all_labels = existing_labels
        else:
            # ãƒ†ã‚¹ãƒˆã‚„KPTã®å ´åˆã¯å¾“æ¥é€šã‚Š
            all_labels = list(set(existing_labels + labels))
        
        issue_data = {
            'title': numbered_title,
            'body': body,
            'labels': all_labels
        }
        
        issue_requests.append((issue_data, issue_type))
    
    return issue_requests

def retry_failed_issues(failed_issues: List[Tuple], max_retry_rounds: int = 2) -> List[Dict]:
    """å¤±æ•—ã—ãŸissueã‚’ãƒªãƒˆãƒ©ã‚¤ã™ã‚‹"""
    if not failed_issues:
        return []
    
    print(f"\nğŸ”„ Retrying {len(failed_issues)} failed issues...")
    
    retry_created = []
    remaining_failed = failed_issues.copy()
    
    for round_num in range(max_retry_rounds):
        if not remaining_failed:
            break
            
        print(f"  ğŸ” Retry round {round_num + 1}/{max_retry_rounds}: {len(remaining_failed)} issues")
        
        # ãƒªãƒˆãƒ©ã‚¤å‰ã«é•·ã‚ã®ä¼‘æ†©
        time.sleep(3.0)
        
        current_round_created, current_round_failed = create_issues_batch(
            remaining_failed, round_num + 1, max_retry_rounds
        )
        
        retry_created.extend(current_round_created)
        remaining_failed = current_round_failed
        
        # æ¬¡ã®ãƒ©ã‚¦ãƒ³ãƒ‰ã¾ã§ã®ä¼‘æ†©
        if remaining_failed and round_num < max_retry_rounds - 1:
            print(f"    â³ Waiting before next retry round...")
            time.sleep(5.0)
    
    if remaining_failed:
        print(f"  âš ï¸ {len(remaining_failed)} issues could not be created after all retries")
        print("  Failed issues:")
        for issue_data, issue_type in remaining_failed[:5]:  # æœ€åˆã®5å€‹ã ã‘è¡¨ç¤º
            print(f"    - {issue_type}: {issue_data['title'][:50]}...")
        if len(remaining_failed) > 5:
            print(f"    ... and {len(remaining_failed) - 5} more")
    
    print(f"  âœ… Retry success: {len(retry_created)} issues created")
    return retry_created

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=" * 60)
    print("ğŸ§  SMART ALL-IN-ONE ISSUE CREATOR v4.3 (sequential + numbered)")
    print("=" * 60)
    print(f"ğŸ“¦ Repository: {GITHUB_REPOSITORY}")
    print(f"â° Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ”§ Script: create_all_issues_smart.py v4.3")
    print(f"âš™ï¸ Configuration:")
    print(f"  â€¢ Parallel Workers: {PARALLEL_WORKERS}")
    print(f"  â€¢ Request Delay: {REQUEST_DELAY}s")
    print(f"  â€¢ Retry Delay: {RETRY_DELAY}s")
    print(f"  â€¢ Max Retries: {MAX_RETRIES}")
    print(f"  â€¢ Batch Size: {BATCH_SIZE}")
    print(f"  â€¢ Dependencies: Standard library only")
    print("=" * 60)
    
    start_time = time.time()
    
    try:
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        task_data, test_data, kpt_data = load_all_csv_data()
        total_issues = len(task_data) + len(test_data) + len(kpt_data)
        
        if total_issues == 0:
            print("âš ï¸ No issues found in CSV files")
            return 1
        
        # ãƒãƒƒãƒè¨ˆç®—
        total_batches = calculate_batches(total_issues, BATCH_SIZE)
        print(f"\nğŸ“Š Processing plan:")
        print(f"  â€¢ Total issues: {total_issues}")
        print(f"  â€¢ Batch size: {BATCH_SIZE}")
        print(f"  â€¢ Total batches: {total_batches}")
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆIDã‚’èª­ã¿è¾¼ã¿
        project_ids = load_project_ids()
        
        # Issueä½œæˆç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™
        task_requests = prepare_issue_data(task_data, [])  # é›£æ˜“åº¦ã‚’ãƒ©ãƒ™ãƒ«ã¨ã—ã¦ä½¿ç”¨
        test_requests = prepare_issue_data(test_data, ['test', 'qa'])
        kpt_requests = prepare_issue_data(kpt_data, ['kpt', 'retrospective'])
        all_requests = task_requests + test_requests + kpt_requests
        
        print(f"\nğŸ“‹ Prepared requests: {len(all_requests)} issues")
        
        # ãƒãƒƒãƒå‡¦ç†
        all_created_issues = []
        task_created = []
        test_created = []
        kpt_created = []
        
        for batch_num in range(total_batches):
            start_idx = batch_num * BATCH_SIZE
            end_idx = min(start_idx + BATCH_SIZE, len(all_requests))
            batch_requests = all_requests[start_idx:end_idx]
            
            print(f"\nğŸ”„ Batch {batch_num + 1}/{total_batches}: Processing issues {start_idx + 1}-{end_idx}")
            
            batch_created, batch_failed = create_issues_batch(batch_requests, batch_num + 1, total_batches)
            all_created_issues.extend(batch_created)
            
            # å¤±æ•—ã—ãŸã‚‚ã®ã‚’é›†ç´„
            if batch_failed:
                print(f"  ğŸ“ {len(batch_failed)} issues failed in this batch, will retry later...")
                if 'all_failed_issues' not in locals():
                    all_failed_issues = []
                all_failed_issues.extend(batch_failed)
            
            # ã‚¿ã‚¹ã‚¯/ãƒ†ã‚¹ãƒˆ/KPTåˆ¥ã«åˆ†é¡
            for issue in batch_created:
                issue_labels = [label['name'] for label in issue.get('labels', [])]
                if 'task' in issue_labels:
                    task_created.append(issue)
                elif 'kpt' in issue_labels:
                    kpt_created.append(issue)
                else:
                    test_created.append(issue)
            
            # ãƒãƒƒãƒé–“ã®ä¼‘æ†©ï¼ˆé•·ã‚ã«ï¼‰
            if batch_num < total_batches - 1:
                print(f"  â³ Batch pause...")
                time.sleep(15)  # 15ç§’ã«å¢—åŠ 
        
        # å¤±æ•—ã—ãŸã‚‚ã®ã®ãƒªãƒˆãƒ©ã‚¤
        retry_created = []
        if 'all_failed_issues' in locals() and all_failed_issues:
            retry_created = retry_failed_issues(all_failed_issues)
            all_created_issues.extend(retry_created)
            
            # ãƒªãƒˆãƒ©ã‚¤ã§ä½œæˆã•ã‚ŒãŸã‚‚ã®ã‚‚åˆ†é¡
            for issue in retry_created:
                issue_labels = [label['name'] for label in issue.get('labels', [])]
                if 'task' in issue_labels:
                    task_created.append(issue)
                elif 'kpt' in issue_labels:
                    kpt_created.append(issue)
                else:
                    test_created.append(issue)
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒªãƒ³ã‚¯
        task_linked, test_linked, kpt_linked = link_issues_to_projects(task_created, test_created, kpt_created, project_ids)
        
        # çµæœã‚µãƒãƒªãƒ¼
        end_time = time.time()
        execution_time = end_time - start_time
        
        print(f"\n" + "=" * 60)
        print("ğŸ‰ SMART PROCESSING COMPLETED!")
        print("=" * 60)
        print(f"ğŸ“Š Results:")
        print(f"  â€¢ Task issues created: {len(task_created)}")
        print(f"  â€¢ Test issues created: {len(test_created)}")
        print(f"  â€¢ KPT issues created: {len(kpt_created)}")
        print(f"  â€¢ Total issues created: {len(all_created_issues)}")
        if retry_created:
            print(f"  â€¢ Retry issues created: {len(retry_created)}")
        print(f"  â€¢ Task issues linked: {task_linked}")
        print(f"  â€¢ Test issues linked: {test_linked}")
        print(f"  â€¢ KPT issues linked: {kpt_linked}")
        final_failed = len(all_failed_issues) - len(retry_created) if 'all_failed_issues' in locals() else 0
        if final_failed > 0:
            print(f"  â€¢ Final failed issues: {final_failed}")
        print(f"  â€¢ Success rate: {(len(all_created_issues)/total_issues*100):.1f}%")
        print(f"â±ï¸ Performance:")
        print(f"  â€¢ Execution time: {execution_time:.1f} seconds")
        print(f"  â€¢ Average per issue: {(execution_time/len(all_created_issues)):.2f}s")
        
        # çµæœä¿å­˜
        with open('smart_issue_creation_result.txt', 'w', encoding='utf-8') as f:
            f.write(f"Smart Issue Creation Results\n")
            f.write(f"Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Task issues: {len(task_created)}\n")
            f.write(f"Test issues: {len(test_created)}\n")
            f.write(f"KPT issues: {len(kpt_created)}\n")
            f.write(f"Total: {len(all_created_issues)}\n")
            if retry_created:
                f.write(f"Retry issues: {len(retry_created)}\n")
            final_failed = len(all_failed_issues) - len(retry_created) if 'all_failed_issues' in locals() else 0
            if final_failed > 0:
                f.write(f"Final failed issues: {final_failed}\n")
            f.write(f"Execution time: {execution_time:.1f}s\n")
            f.write(f"Success rate: {(len(all_created_issues)/total_issues*100):.1f}%\n")
        
        return 0
        
    except Exception as e:
        print(f"\nğŸ’¥ Unexpected error: {str(e)}")
        print(f"ğŸ”§ Error type: {type(e).__name__}")
        return 1

if __name__ == '__main__':
    exit(main())
