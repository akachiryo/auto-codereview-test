#!/usr/bin/env python3
"""
GitHub Issues å…¨è‡ªå‹•ä½œæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ v4.1 (SMART)
ã™ã¹ã¦ã®Issueã‚’1ã¤ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§å‹•çš„ã«å‡¦ç†
ä¾å­˜é–¢ä¿‚ï¼šæ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã¿ä½¿ç”¨
"""

import os
import requests
import csv
import time
import sys
from typing import Dict, List, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import math

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’å–å¾—
TEAM_SETUP_TOKEN = os.environ.get('TEAM_SETUP_TOKEN')
GITHUB_REPOSITORY = os.environ.get('GITHUB_REPOSITORY')

# å‹•çš„è¨­å®š
PARALLEL_WORKERS = 6     # ä¸¦åˆ—æ•°ã‚’å°‘ã—ä¸‹ã’ã¦å®‰å®šæ€§å‘ä¸Š
REQUEST_DELAY = 0.2      # å°‘ã—é•·ã‚ã«ã—ã¦Rate Limitå›é¿
BATCH_SIZE = 50          # ãƒãƒƒãƒã‚µã‚¤ã‚º
BURST_LIMIT = 15         # ãƒãƒ¼ã‚¹ãƒˆãƒªãƒŸãƒƒãƒˆ

if not TEAM_SETUP_TOKEN or not GITHUB_REPOSITORY:
    raise ValueError("TEAM_SETUP_TOKEN and GITHUB_REPOSITORY environment variables are required")

REPO_OWNER, REPO_NAME = GITHUB_REPOSITORY.split('/')

# GitHub APIè¨­å®š
API_BASE = 'https://api.github.com'
REST_HEADERS = {
    'Authorization': f'token {TEAM_SETUP_TOKEN}',
    'Accept': 'application/vnd.github.v3+json',
    'X-GitHub-Api-Version': '2022-11-28'
}

# GraphQL APIè¨­å®š
GRAPHQL_URL = 'https://api.github.com/graphql'
GRAPHQL_HEADERS = {
    'Authorization': f'Bearer {TEAM_SETUP_TOKEN}',
    'Content-Type': 'application/json'
}

# ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ­ãƒ¼ã‚«ãƒ«ã‚»ãƒƒã‚·ãƒ§ãƒ³
thread_local = threading.local()

def get_session():
    """ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ­ãƒ¼ã‚«ãƒ«ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’å–å¾—"""
    if not hasattr(thread_local, "session"):
        thread_local.session = requests.Session()
        thread_local.session.headers.update(REST_HEADERS)
    return thread_local.session

def load_all_csv_data() -> Tuple[List[Dict], List[Dict]]:
    """å…¨ã¦ã®CSVãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
    print("ğŸ“Š Loading all CSV data...")
    
    # ã‚¿ã‚¹ã‚¯Issues
    task_issues = []
    task_csv_path = 'data/tasks_for_issues.csv'
    if os.path.exists(task_csv_path):
        with open(task_csv_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            task_issues = [row for row in reader if row.get('title', '').strip()]
    
    # ãƒ†ã‚¹ãƒˆIssues
    test_issues = []
    test_csv_path = 'data/tests_for_issues.csv'
    if os.path.exists(test_csv_path):
        with open(test_csv_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            test_issues = [row for row in reader if row.get('title', '').strip()]
    
    print(f"ğŸ“‹ Loaded: {len(task_issues)} task issues, {len(test_issues)} test issues")
    print(f"ğŸ“Š Total: {len(task_issues) + len(test_issues)} issues to create")
    
    return task_issues, test_issues

def calculate_batches(total_count: int, batch_size: int) -> int:
    """å¿…è¦ãªãƒãƒƒãƒæ•°ã‚’è¨ˆç®—"""
    return math.ceil(total_count / batch_size)

def create_single_issue(issue_data: Dict, index: int, total: int, issue_type: str) -> Optional[Dict]:
    """å˜ä¸€ã®Issueã‚’ä½œæˆ"""
    session = get_session()
    
    try:
        if index > 0:
            time.sleep(REQUEST_DELAY)
        
        response = session.post(
            f"{API_BASE}/repos/{GITHUB_REPOSITORY}/issues",
            json=issue_data,
            timeout=30
        )
        
        if response.status_code == 201:
            issue = response.json()
            print(f"  âœ… {issue_type} ({index + 1}/{total}): {issue_data['title'][:50]}...")
            return issue
        elif response.status_code == 403:
            print(f"  â³ Rate limit hit, retrying ({index + 1}/{total})...")
            time.sleep(3)
            response = session.post(
                f"{API_BASE}/repos/{GITHUB_REPOSITORY}/issues",
                json=issue_data,
                timeout=30
            )
            if response.status_code == 201:
                issue = response.json()
                print(f"  âœ… {issue_type} retry ({index + 1}/{total}): {issue_data['title'][:50]}...")
                return issue
        
        print(f"  âŒ {issue_type} failed ({index + 1}/{total}): {response.status_code}")
        return None
        
    except Exception as e:
        print(f"  âŒ {issue_type} exception ({index + 1}/{total}): {str(e)}")
        return None

def create_issues_batch(issues_data: List[Tuple], batch_num: int, total_batches: int) -> List[Dict]:
    """1ã¤ã®ãƒãƒƒãƒã§Issueã‚’ä½œæˆ"""
    created_issues = []
    
    if not issues_data:
        return created_issues
    
    print(f"ğŸš€ Processing batch {batch_num}/{total_batches} ({len(issues_data)} issues)")
    
    # ä¸¦åˆ—å®Ÿè¡Œ
    with ThreadPoolExecutor(max_workers=PARALLEL_WORKERS) as executor:
        futures = []
        for i, (issue_data, issue_type) in enumerate(issues_data):
            future = executor.submit(create_single_issue, issue_data, i, len(issues_data), issue_type)
            futures.append(future)
            
            # ãƒãƒ¼ã‚¹ãƒˆãƒªãƒŸãƒƒãƒˆ
            if i > 0 and i % BURST_LIMIT == 0:
                time.sleep(0.5)
        
        # çµæœåé›†
        for future in as_completed(futures):
            try:
                issue = future.result(timeout=60)
                if issue:
                    created_issues.append(issue)
            except Exception as e:
                print(f"  âŒ Future exception: {str(e)}")
    
    print(f"ğŸ“Š Batch {batch_num} result: {len(created_issues)}/{len(issues_data)} issues created")
    return created_issues

def add_issue_to_project_fast(project_id: str, issue: Dict) -> bool:
    """é«˜é€Ÿã§Issueã‚’Projectã«è¿½åŠ """
    query = """
    mutation($projectId: ID!, $contentId: ID!) {
        addProjectV2ItemById(input: {projectId: $projectId, contentId: $contentId}) {
            item { id }
        }
    }
    """
    
    variables = {
        'projectId': project_id,
        'contentId': issue['node_id']
    }
    
    payload = {'query': query, 'variables': variables}
    
    try:
        response = requests.post(
            GRAPHQL_URL, 
            json=payload, 
            headers=GRAPHQL_HEADERS,
            timeout=30
        )
        
        if response.status_code == 200:
            data = response.json()
            if 'errors' not in data and 'data' in data:
                return True
        return False
    except:
        return False

def link_issues_to_projects(task_issues: List[Dict], test_issues: List[Dict], project_ids: Dict[str, str]):
    """Issueã‚’Projectsã«ãƒªãƒ³ã‚¯"""
    print("\nğŸ”— Linking issues to projects...")
    
    def link_batch(issues: List[Dict], project_id: str, project_name: str, issue_type: str):
        if not issues or not project_id:
            return 0
        
        print(f"  ğŸ“Œ Linking {len(issues)} {issue_type} issues to {project_name}")
        success_count = 0
        
        with ThreadPoolExecutor(max_workers=PARALLEL_WORKERS) as executor:
            futures = []
            for issue in issues:
                future = executor.submit(add_issue_to_project_fast, project_id, issue)
                futures.append(future)
            
            for i, future in enumerate(as_completed(futures)):
                try:
                    if future.result(timeout=30):
                        success_count += 1
                    if (i + 1) % 20 == 0:
                        print(f"    âœ… Linked {i + 1}/{len(issues)} to {project_name}")
                except Exception as e:
                    print(f"    âŒ Link exception: {str(e)}")
        
        print(f"  ğŸ“Š {project_name}: {success_count}/{len(issues)} issues linked")
        return success_count
    
    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«ãƒªãƒ³ã‚¯
    task_linked = link_batch(
        task_issues, 
        project_ids.get('ã‚¤ãƒã‚³ã‚³SNSï¼ˆã‚¿ã‚¹ã‚¯ï¼‰'), 
        'ã‚¤ãƒã‚³ã‚³SNSï¼ˆã‚¿ã‚¹ã‚¯ï¼‰',
        'task'
    )
    
    test_linked = link_batch(
        test_issues,
        project_ids.get('ã‚¤ãƒã‚³ã‚³SNSï¼ˆãƒ†ã‚¹ãƒˆï¼‰'),
        'ã‚¤ãƒã‚³ã‚³SNSï¼ˆãƒ†ã‚¹ãƒˆï¼‰',
        'test'
    )
    
    return task_linked, test_linked

def load_project_ids() -> Dict[str, str]:
    """ä¿å­˜ã•ã‚ŒãŸãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆIDã‚’èª­ã¿è¾¼ã¿"""
    project_ids = {}
    try:
        with open('project_ids.txt', 'r', encoding='utf-8') as f:
            for line in f:
                if ':' in line:
                    title, project_id = line.strip().split(':', 1)
                    project_ids[title] = project_id
        print(f"ğŸ“‚ Loaded {len(project_ids)} project IDs")
    except FileNotFoundError:
        print("âš ï¸ project_ids.txt not found. Issues will be created but not linked to projects.")
    
    return project_ids

def prepare_issue_data(issues: List[Dict], labels: List[str]) -> List[Tuple[Dict, str]]:
    """Issueä½œæˆç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™"""
    issue_requests = []
    for row in issues:
        title = row.get('title', '').strip()
        body = row.get('body', '').strip()
        
        if not title:
            continue
            
        existing_labels = [label.strip() for label in row.get('labels', '').split(',') if label.strip()]
        all_labels = list(set(existing_labels + labels))
        
        issue_data = {
            'title': title,
            'body': body,
            'labels': all_labels
        }
        
        issue_type = 'task' if 'task' in labels else 'test'
        issue_requests.append((issue_data, issue_type))
    
    return issue_requests

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=" * 60)
    print("ğŸ§  SMART ALL-IN-ONE ISSUE CREATOR v4.1")
    print("=" * 60)
    print(f"ğŸ“¦ Repository: {GITHUB_REPOSITORY}")
    print(f"â° Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ”§ Script: create_all_issues_smart.py v4.1")
    print(f"âš™ï¸ Configuration:")
    print(f"  â€¢ Parallel Workers: {PARALLEL_WORKERS}")
    print(f"  â€¢ Request Delay: {REQUEST_DELAY}s")
    print(f"  â€¢ Batch Size: {BATCH_SIZE}")
    print(f"  â€¢ Dependencies: Standard library only")
    print("=" * 60)
    
    start_time = time.time()
    
    try:
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        task_data, test_data = load_all_csv_data()
        total_issues = len(task_data) + len(test_data)
        
        if total_issues == 0:
            print("âš ï¸ No issues found in CSV files")
            return 1
        
        # ãƒãƒƒãƒè¨ˆç®—
        total_batches = calculate_batches(total_issues, BATCH_SIZE)
        print(f"\nğŸ“Š Processing plan:")
        print(f"  â€¢ Total issues: {total_issues}")
        print(f"  â€¢ Batch size: {BATCH_SIZE}")
        print(f"  â€¢ Total batches: {total_batches}")
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆIDã‚’èª­ã¿è¾¼ã¿
        project_ids = load_project_ids()
        
        # Issueä½œæˆç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™
        task_requests = prepare_issue_data(task_data, ['task', 'development'])
        test_requests = prepare_issue_data(test_data, ['test', 'qa'])
        all_requests = task_requests + test_requests
        
        print(f"\nğŸ“‹ Prepared requests: {len(all_requests)} issues")
        
        # ãƒãƒƒãƒå‡¦ç†
        all_created_issues = []
        task_created = []
        test_created = []
        
        for batch_num in range(total_batches):
            start_idx = batch_num * BATCH_SIZE
            end_idx = min(start_idx + BATCH_SIZE, len(all_requests))
            batch_requests = all_requests[start_idx:end_idx]
            
            print(f"\nğŸ”„ Batch {batch_num + 1}/{total_batches}: Processing issues {start_idx + 1}-{end_idx}")
            
            batch_created = create_issues_batch(batch_requests, batch_num + 1, total_batches)
            all_created_issues.extend(batch_created)
            
            # ã‚¿ã‚¹ã‚¯/ãƒ†ã‚¹ãƒˆåˆ¥ã«åˆ†é¡
            for issue in batch_created:
                issue_labels = [label['name'] for label in issue.get('labels', [])]
                if 'task' in issue_labels:
                    task_created.append(issue)
                else:
                    test_created.append(issue)
            
            # ãƒãƒƒãƒé–“ã®ä¼‘æ†©
            if batch_num < total_batches - 1:
                print(f"  â³ Batch pause...")
                time.sleep(2)
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒªãƒ³ã‚¯
        task_linked, test_linked = link_issues_to_projects(task_created, test_created, project_ids)
        
        # çµæœã‚µãƒãƒªãƒ¼
        end_time = time.time()
        execution_time = end_time - start_time
        
        print(f"\n" + "=" * 60)
        print("ğŸ‰ SMART PROCESSING COMPLETED!")
        print("=" * 60)
        print(f"ğŸ“Š Results:")
        print(f"  â€¢ Task issues created: {len(task_created)}")
        print(f"  â€¢ Test issues created: {len(test_created)}")
        print(f"  â€¢ Total issues created: {len(all_created_issues)}")
        print(f"  â€¢ Task issues linked: {task_linked}")
        print(f"  â€¢ Test issues linked: {test_linked}")
        print(f"  â€¢ Success rate: {(len(all_created_issues)/total_issues*100):.1f}%")
        print(f"â±ï¸ Performance:")
        print(f"  â€¢ Execution time: {execution_time:.1f} seconds")
        print(f"  â€¢ Average per issue: {(execution_time/len(all_created_issues)):.2f}s")
        
        # çµæœä¿å­˜
        with open('smart_issue_creation_result.txt', 'w', encoding='utf-8') as f:
            f.write(f"Smart Issue Creation Results\n")
            f.write(f"Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Task issues: {len(task_created)}\n")
            f.write(f"Test issues: {len(test_created)}\n")
            f.write(f"Total: {len(all_created_issues)}\n")
            f.write(f"Execution time: {execution_time:.1f}s\n")
            f.write(f"Success rate: {(len(all_created_issues)/total_issues*100):.1f}%\n")
        
        return 0
        
    except Exception as e:
        print(f"\nğŸ’¥ Unexpected error: {str(e)}")
        print(f"ğŸ”§ Error type: {type(e).__name__}")
        return 1

if __name__ == '__main__':
    exit(main())