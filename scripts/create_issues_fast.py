#!/usr/bin/env python3
"""
GitHub Issues é«˜é€Ÿãƒãƒƒãƒä½œæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ v4.0
ä¸¦åˆ—å‡¦ç†ã«ã‚ˆã‚ŠIssueä½œæˆã‚’å¤§å¹…ã«é«˜é€ŸåŒ–
"""

import os
import requests
import csv
import time
import sys
import asyncio
import aiohttp
from typing import Dict, List, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’å–å¾—
TEAM_SETUP_TOKEN = os.environ.get('TEAM_SETUP_TOKEN')
GITHUB_REPOSITORY = os.environ.get('GITHUB_REPOSITORY')
BATCH_NUMBER = int(os.environ.get('BATCH_NUMBER', '1'))
BATCH_SIZE = int(os.environ.get('BATCH_SIZE', '50'))

# é«˜é€ŸåŒ–è¨­å®š
PARALLEL_WORKERS = 8  # åŒæ™‚ä¸¦åˆ—ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°
REQUEST_DELAY = 0.1   # ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”ï¼ˆç§’ï¼‰- å¤§å¹…çŸ­ç¸®
BURST_LIMIT = 20      # ä¸€åº¦ã«é€ä¿¡ã™ã‚‹ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°

if not TEAM_SETUP_TOKEN or not GITHUB_REPOSITORY:
    raise ValueError("TEAM_SETUP_TOKEN and GITHUB_REPOSITORY environment variables are required")

REPO_OWNER, REPO_NAME = GITHUB_REPOSITORY.split('/')

# GitHub APIè¨­å®š
API_BASE = 'https://api.github.com'
REST_HEADERS = {
    'Authorization': f'token {TEAM_SETUP_TOKEN}',
    'Accept': 'application/vnd.github.v3+json',
    'X-GitHub-Api-Version': '2022-11-28'
}

# GraphQL APIè¨­å®š
GRAPHQL_URL = 'https://api.github.com/graphql'
GRAPHQL_HEADERS = {
    'Authorization': f'Bearer {TEAM_SETUP_TOKEN}',
    'Content-Type': 'application/json'
}

# ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ­ãƒ¼ã‚«ãƒ«ã‚»ãƒƒã‚·ãƒ§ãƒ³
thread_local = threading.local()

def get_session():
    """ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ­ãƒ¼ã‚«ãƒ«ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’å–å¾—"""
    if not hasattr(thread_local, "session"):
        thread_local.session = requests.Session()
        thread_local.session.headers.update(REST_HEADERS)
    return thread_local.session

def create_single_issue(issue_data: Dict, index: int, total: int) -> Optional[Dict]:
    """å˜ä¸€ã®Issueã‚’ä½œæˆï¼ˆä¸¦åˆ—å‡¦ç†ç”¨ï¼‰"""
    session = get_session()
    
    try:
        # çŸ­ã„å¾…æ©Ÿã§Rate limitå›é¿
        if index > 0:
            time.sleep(REQUEST_DELAY)
        
        response = session.post(
            f"{API_BASE}/repos/{GITHUB_REPOSITORY}/issues",
            json=issue_data,
            timeout=30
        )
        
        if response.status_code == 201:
            issue = response.json()
            print(f"  âœ… Created ({index + 1}/{total}): {issue_data['title'][:50]}...")
            return issue
        elif response.status_code == 403:
            print(f"  â³ Rate limit hit, retrying ({index + 1}/{total})...")
            time.sleep(2)
            # ãƒªãƒˆãƒ©ã‚¤
            response = session.post(
                f"{API_BASE}/repos/{GITHUB_REPOSITORY}/issues",
                json=issue_data,
                timeout=30
            )
            if response.status_code == 201:
                issue = response.json()
                print(f"  âœ… Retry success ({index + 1}/{total}): {issue_data['title'][:50]}...")
                return issue
        
        print(f"  âŒ Failed ({index + 1}/{total}): {response.status_code} - {response.text[:100]}")
        return None
        
    except Exception as e:
        print(f"  âŒ Exception ({index + 1}/{total}): {str(e)}")
        return None

def create_issues_parallel(batch_data: List[Dict], issue_type: str, labels: List[str]) -> List[Dict]:
    """ä¸¦åˆ—å‡¦ç†ã§Issuesã‚’ä½œæˆ"""
    created_issues = []
    
    print(f"ğŸš€ Creating {len(batch_data)} {issue_type} issues with {PARALLEL_WORKERS} parallel workers...")
    
    # Issueä½œæˆãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
    issue_requests = []
    for i, row in enumerate(batch_data):
        title = row.get('title', '').strip()
        body = row.get('body', '').strip()
        
        if not title:  # ã‚¿ã‚¤ãƒˆãƒ«ãŒç©ºã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            print(f"  âš ï¸ Skipping row {i + 1}: empty title")
            continue
            
        existing_labels = [label.strip() for label in row.get('labels', '').split(',') if label.strip()]
        all_labels = list(set(existing_labels + labels))
        
        issue_data = {
            'title': title,
            'body': body,
            'labels': all_labels
        }
        issue_requests.append((issue_data, i, len(batch_data)))
    
    if not issue_requests:
        print("  âš ï¸ No valid issues to create")
        return []
    
    # ä¸¦åˆ—å®Ÿè¡Œ
    with ThreadPoolExecutor(max_workers=PARALLEL_WORKERS) as executor:
        # ãƒãƒ¼ã‚¹ãƒˆã‚’é¿ã‘ã‚‹ãŸã‚ã€ä¸€å®šé–“éš”ã§ã‚¿ã‚¹ã‚¯ã‚’æŠ•å…¥
        futures = []
        for i, (issue_data, index, total) in enumerate(issue_requests):
            future = executor.submit(create_single_issue, issue_data, index, total)
            futures.append(future)
            
            # ãƒãƒ¼ã‚¹ãƒˆãƒªãƒŸãƒƒãƒˆé©ç”¨
            if i > 0 and i % BURST_LIMIT == 0:
                print(f"  ğŸ”„ Burst limit reached, brief pause at {i}/{len(issue_requests)}...")
                time.sleep(1)
        
        # çµæœã‚’åé›†
        for future in as_completed(futures):
            try:
                issue = future.result(timeout=60)
                if issue:
                    created_issues.append(issue)
            except Exception as e:
                print(f"  âŒ Future exception: {str(e)}")
    
    print(f"ğŸ¯ Parallel creation completed: {len(created_issues)}/{len(issue_requests)} issues created")
    return created_issues

def add_issue_to_project_fast(project_id: str, issue: Dict) -> bool:
    """é«˜é€Ÿã§Issueã‚’Projectã«è¿½åŠ """
    query = """
    mutation($projectId: ID!, $contentId: ID!) {
        addProjectV2ItemById(input: {projectId: $projectId, contentId: $contentId}) {
            item { id }
        }
    }
    """
    
    variables = {
        'projectId': project_id,
        'contentId': issue['node_id']
    }
    
    payload = {'query': query, 'variables': variables}
    
    try:
        response = requests.post(
            GRAPHQL_URL, 
            json=payload, 
            headers=GRAPHQL_HEADERS,
            timeout=30
        )
        
        if response.status_code == 200:
            data = response.json()
            if 'errors' not in data and 'data' in data:
                return True
        
        return False
    except:
        return False

def add_issues_to_project_parallel(project_id: str, issues: List[Dict], project_name: str):
    """ä¸¦åˆ—å‡¦ç†ã§Issuesã‚’Projectsã«è¿½åŠ """
    if not issues or not project_id:
        return
    
    print(f"ğŸ”— Adding {len(issues)} issues to project: {project_name} (parallel)")
    
    success_count = 0
    
    def add_single_issue(issue, index, total):
        nonlocal success_count
        try:
            if add_issue_to_project_fast(project_id, issue):
                success_count += 1
                if index % 10 == 0:  # 10ä»¶ã”ã¨ã«é€²æ—è¡¨ç¤º
                    print(f"    âœ… Progress: {index + 1}/{total} linked to project")
                return True
            else:
                print(f"    âŒ Failed to link: {issue['title'][:40]}...")
                return False
        except Exception as e:
            print(f"    âŒ Exception linking: {str(e)}")
            return False
    
    # ä¸¦åˆ—å®Ÿè¡Œã§ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¿½åŠ 
    with ThreadPoolExecutor(max_workers=PARALLEL_WORKERS) as executor:
        futures = []
        for i, issue in enumerate(issues):
            if i > 0 and i % BURST_LIMIT == 0:
                time.sleep(0.5)  # GraphQL rate limitå¯¾ç­–
            
            future = executor.submit(add_single_issue, issue, i, len(issues))
            futures.append(future)
        
        # å®Œäº†ã‚’å¾…ã¤
        for future in as_completed(futures):
            try:
                future.result(timeout=30)
            except Exception as e:
                print(f"    âŒ Project linking exception: {str(e)}")
    
    print(f"ğŸ”— Project linking completed: {success_count}/{len(issues)} issues linked to {project_name}")

def load_project_ids() -> Dict[str, str]:
    """ä¿å­˜ã•ã‚ŒãŸãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆIDã‚’èª­ã¿è¾¼ã¿"""
    project_ids = {}
    try:
        with open('project_ids.txt', 'r', encoding='utf-8') as f:
            for line in f:
                if ':' in line:
                    title, project_id = line.strip().split(':', 1)
                    project_ids[title] = project_id
        print(f"ğŸ“‚ Loaded {len(project_ids)} project IDs")
    except FileNotFoundError:
        print("âš ï¸ project_ids.txt not found. Issues will be created but not linked to projects.")
    
    return project_ids

def get_csv_batch(csv_path: str, batch_number: int, batch_size: int) -> List[Dict]:
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æŒ‡å®šãƒãƒƒãƒã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    if not os.path.exists(csv_path):
        print(f"âš ï¸ CSV file not found: {csv_path}")
        return []
    
    start_index = (batch_number - 1) * batch_size
    end_index = start_index + batch_size
    
    print(f"ğŸ“Š Reading batch {batch_number}: rows {start_index + 1} to {end_index}")
    
    try:
        with open(csv_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            all_rows = list(reader)
            batch_rows = all_rows[start_index:end_index]
            
            print(f"ğŸ“‹ Total rows in CSV: {len(all_rows)}")
            print(f"ğŸ“‹ Batch {batch_number} rows: {len(batch_rows)}")
            
            return batch_rows
            
    except Exception as e:
        print(f"âŒ Error reading CSV {csv_path}: {str(e)}")
        return []

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=" * 60)
    print(f"ğŸš€ FAST PARALLEL ISSUE CREATION v4.0")
    print("=" * 60)
    print(f"ğŸ“¦ Repository: {GITHUB_REPOSITORY}")
    print(f"â° Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ”§ Script: create_issues_fast.py v4.0")
    print(f"ğŸ“Š Performance Configuration:")
    print(f"  â€¢ Parallel Workers: {PARALLEL_WORKERS}")
    print(f"  â€¢ Request Delay: {REQUEST_DELAY}s (vs old: 2s)")
    print(f"  â€¢ Burst Limit: {BURST_LIMIT}")
    print(f"  â€¢ Batch Number: {BATCH_NUMBER}")
    print(f"  â€¢ Batch Size: {BATCH_SIZE}")
    print("=" * 60)
    
    start_time = time.time()
    
    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆIDã‚’èª­ã¿è¾¼ã¿
    project_ids = load_project_ids()
    
    # ã‚¿ã‚¹ã‚¯Issuesã®ãƒãƒƒãƒå‡¦ç†
    print(f"\nğŸ“ Processing task issues batch {BATCH_NUMBER} (PARALLEL)...")
    task_batch = get_csv_batch('data/tasks_for_issues.csv', BATCH_NUMBER, BATCH_SIZE)
    task_issues = []
    
    if task_batch:
        task_issues = create_issues_parallel(
            task_batch,
            'task',
            ['task', 'development']
        )
        
        # ã‚¿ã‚¹ã‚¯ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«ç´ä»˜ã‘ï¼ˆä¸¦åˆ—ï¼‰
        task_project_id = project_ids.get('ã‚¤ãƒã‚³ã‚³SNSï¼ˆã‚¿ã‚¹ã‚¯ï¼‰')
        if task_project_id and task_issues:
            add_issues_to_project_parallel(task_project_id, task_issues, 'ã‚¤ãƒã‚³ã‚³SNSï¼ˆã‚¿ã‚¹ã‚¯ï¼‰')
    
    # ãƒ†ã‚¹ãƒˆIssuesã®ãƒãƒƒãƒå‡¦ç†
    print(f"\nğŸ§ª Processing test issues batch {BATCH_NUMBER} (PARALLEL)...")
    test_batch = get_csv_batch('data/tests_for_issues.csv', BATCH_NUMBER, BATCH_SIZE)
    test_issues = []
    
    if test_batch:
        test_issues = create_issues_parallel(
            test_batch, 
            'test',
            ['test', 'qa']
        )
        
        # ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«ç´ä»˜ã‘ï¼ˆä¸¦åˆ—ï¼‰
        test_project_id = project_ids.get('ã‚¤ãƒã‚³ã‚³SNSï¼ˆãƒ†ã‚¹ãƒˆï¼‰')
        if test_project_id and test_issues:
            add_issues_to_project_parallel(test_project_id, test_issues, 'ã‚¤ãƒã‚³ã‚³SNSï¼ˆãƒ†ã‚¹ãƒˆï¼‰')
    
    # å®Ÿè¡Œæ™‚é–“è¨ˆç®—
    end_time = time.time()
    execution_time = end_time - start_time
    
    print(f"\nâš¡ FAST BATCH {BATCH_NUMBER} COMPLETED!")
    print(f"ğŸ“Œ Results:")
    print(f"  â€¢ {len(task_issues)} task issues created")
    print(f"  â€¢ {len(test_issues)} test issues created")
    print(f"  â€¢ Total execution time: {execution_time:.1f} seconds")
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ
    old_estimated_time = (len(task_issues) + len(test_issues)) * 2 + ((len(task_issues) + len(test_issues)) // 10) * 5
    speedup = old_estimated_time / execution_time if execution_time > 0 else 1
    print(f"  â€¢ Old method would take: ~{old_estimated_time} seconds")
    print(f"  â€¢ Speed improvement: {speedup:.1f}x faster!")
    
    # ãƒãƒƒãƒå®Œäº†çŠ¶æ³ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    with open(f'batch_{BATCH_NUMBER}_completed_fast.txt', 'w', encoding='utf-8') as f:
        f.write(f"Fast Batch {BATCH_NUMBER} completed\n")
        f.write(f"Task issues: {len(task_issues)}\n")
        f.write(f"Test issues: {len(test_issues)}\n")
        f.write(f"Execution time: {execution_time:.1f} seconds\n")
        f.write(f"Speed improvement: {speedup:.1f}x\n")
        f.write(f"Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    return 0

if __name__ == '__main__':
    exit(main())